#!/usr/bin/env python


"""GraphEmbed.

Compute a 2D embedding of a data matrix given supervised class information.
A discrete label for each instance is expected.
A graph is built where nodes are instances and there exist two types
of edges: the 'knn' edges and the 'k_shift' edges.
A knn edge is an edge to the k-th nearest instance that has the same
label.
A k_shift edge is an edge to the k-th nearest instance that is denser
and has a different label.
The density is defined as the sum of the pairwise cosine similarity between
an instance and all the other instances.
The desired edge length is the euclidean distance between the instances.
If the endpoints of an edge have the same label then the desired distance
is divided by 1 + class_bias.
A k-shift edge is deleted if at least one of the endpoints of is an
outlier.
Outlier nodes are defined as those instances that have no mutual
k=knn_outlier neighbors.

Finally the embedding is computed as the 2D coordinates of the
corresponding graph embedding using the force layout algorithm from
Tomihisa Kamada, and Satoru Kawai. "An algorithm for drawing general
undirected graphs.", Information processing letters 31, no. 1 (1989): 7-15.

Version: 2.0
Author: Fabrizio Costa [costa@informatik.uni-freiburg.de]

Usage:
  graph_embed -i FILE (-t FILE | -n N)  [-o NAME] [--cmap_name=NAME]
              [--knn=N] [--k_quick_shift=N] [--knn_outlier=N]
              [--class_bias=N]
              [--correlation_transformation]
              [--min_threshold=N] [--max_threshold=N]
              [--display] [--figure_size=N] [--verbose]
  graph_embed (-h | --help)
  graph_embed --version

Options:
  -i FILE                           Specify input data file in CSV format.
  -t FILE                           Specify classes data file in CSV format.
  --min_threshold=N                 Min num instances per class [default: 5]
  --max_threshold=N                 Max num instances per class [default: 1000]
  -o NAME                           Prefix for output directory [default: out].
  --correlation_transformation      Convert data matrix to corr coeff matrix.
  --knn=N                           Number of neighbors for knn links
                                    [default: 3].
  --k_quick_shift=N                 Number of neighbors for k_shift links
                                    [default: 3].
  --knn_outlier=N                   Number of neighbors for outlier
                                    identification [default: 3].
  --class_bias=N                    Contraction bias for clustering
                                    [default: 1.0].
  --display                         Display graphs on terminal.
  --figure_size=N                   Figure size [default: 15].
  --cmap_name=NAME                  Color scheme [default: gist_ncar].
  -h --help                         Show this screen.
  --version                         Show version.
  --verbose                         Print more text.


"""
import os
import sys
import time
import logging
import logging.handlers
import collections
from docopt import docopt
import numpy as np
from sklearn.preprocessing import LabelEncoder
from graph_layout_embedder import Embedder

logger = logging.getLogger(__name__)


def serialize_dict(the_dict, full=True, offset='small'):
    """serialize_dict."""
    if the_dict:
        text = []
        for key in sorted(the_dict):
            if offset == 'small':
                line = '%10s: %s' % (key, the_dict[key])
            elif offset == 'large':
                line = '%25s: %s' % (key, the_dict[key])
            elif offset == 'very_large':
                line = '%50s: %s' % (key, the_dict[key])
            else:
                raise Exception('unrecognized option: %s' % offset)
            line = line.replace('\n', ' ')
            if full is False:
                if len(line) > 100:
                    line = line[:100] + '  ...  ' + line[-20:]
            text.append(line)
        return '\n'.join(text)
    else:
        return ""


def configure_logging(logger, verbosity=0, filename=None):
    """Utility to configure the logging aspects.

    If filename is None then
    no info is stored in files.
    If filename is not None then everything that is logged is dumped to
    file (including program traces).
    Verbosity is an int that can take values:
    0 -> warning, 1 -> info, >=2 -> debug.
    All levels are displayed on stdout, not on stderr.
    Please use exceptions and asserts
    to output on stderr.
    """
    logger.propagate = False
    logger.handlers = []
    log_level = logging.WARNING
    if verbosity == 1:
        log_level = logging.INFO
    elif verbosity >= 2:
        log_level = logging.DEBUG
    logger.setLevel(logging.DEBUG)
    # create console handler
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(log_level)
    # create formatter
    cformatter = logging.Formatter('%(message)s')
    # add formatter to ch
    ch.setFormatter(cformatter)
    # add handlers to logger
    logger.addHandler(ch)

    if filename is not None:
        # create a file handler
        fh = logging.handlers.RotatingFileHandler(filename=filename,
                                                  maxBytes=10000000,
                                                  backupCount=10)
        fh.setLevel(logging.DEBUG)
        # create formatter
        fformatter = logging.Formatter('%(asctime)s | %(levelname)-6s | %(name)10s | %(filename)10s |\
   %(lineno)4s | %(message)s')
        # add formatter to fh
        fh.setFormatter(fformatter)
        # add handlers to logger
        logger.addHandler(fh)


def _loaddata_matrix(fname):
    logger.info('Reading data from file: %s' % fname)
    data_matrix_original = []
    instance_names = []
    gene_names = []
    with open(fname) as f:
        for i, line in enumerate(f):
            if i == 0:
                instance_names = line.strip().split()[1:]
            if i > 0:
                tokens = line.strip().split('\t')
                gene_names.append(tokens[0])
                value_list = tokens[1:]
                vals = [float(j) for j in value_list]
                data_matrix_original.append(vals)
    data_matrix = np.array(data_matrix_original).T
    rows, cols = data_matrix.shape
    logger.info('#instances:%d  #features:%d' % (rows, cols))
    return data_matrix, gene_names, instance_names


def _load_target(fname):
    logger.info('Reading data from file: %s' % fname)
    targets = []
    with open(fname) as f:
        for line in f:
            tokens = line.strip().split()
            targets.append(tokens[1])
    logger.info('read %d values ' % len(targets))
    target_names = list(sorted(set(targets)))
    lenc = LabelEncoder()
    y = lenc.fit_transform(targets)
    targets = np.array(y)
    return targets, target_names


def _select_targets(y, min_threshold=10, max_threshold=None):
    """_select_targets.

    Return the set of targets that are occurring a number of times bounded
    by min_threshold and max_threshold.
    """
    c = collections.Counter(y)
    y_sel = []
    for y_id in c:
        if c[y_id] > min_threshold:
            if max_threshold and c[y_id] < max_threshold:
                y_sel.append(y_id)
            else:
                y_sel.append(y_id)
    return y_sel


def _filter_dataset(data_matrix, y, y_sel):
    """_filter_dataset.

    Filter data matrix and target vector selecting only instances that
    belong to y_sel.
    """
    targets = []
    instances = []
    for target, instance in zip(y, data_matrix):
        if target in y_sel:
            targets.append(target)
            instances.append(instance)
    y = np.array(np.hstack(targets))
    _data_matrix = np.array(np.vstack(instances))
    return _data_matrix, y


def pre_process(data_fname=None,
                target_fname=None,
                correlation_transformation=None,
                min_threshold=None,
                max_threshold=None):
    """Process data."""
    # load data
    data_matrix, gene_names, instance_names = _loaddata_matrix(data_fname)

    # prepare data matrix
    if correlation_transformation:
        data_matrix = np.corrcoef(data_matrix)

    # prepare target
    y_orig, target_names = _load_target(target_fname)
    y_sel = _select_targets(y_orig,
                            min_threshold=min_threshold,
                            max_threshold=max_threshold)
    logger.info('original num classes: %d' % len(set(y_orig)))
    logger.info('selected %d classes with more than %d instances' %
                (len(y_sel), min_threshold))
    data_matrix, y_orig_sel = _filter_dataset(data_matrix, y_orig, y_sel)
    rows, cols = data_matrix.shape
    logger.info('num instances:%d  num features:%d' % (rows, cols))
    lenc = LabelEncoder()
    y = lenc.fit_transform(y_orig_sel)
    y = np.array(y)
    target_dict = dict()
    for i, c in enumerate(lenc.classes_):
        target_dict[i] = target_names[c]

    return data_matrix, y, target_dict


def main(args):
    """Main."""
    # setup variables
    data_fname = args['-i']
    target_fname = args['-t']
    min_threshold = int(args['--min_threshold'])
    max_threshold = int(args['--max_threshold'])
    cmap_name = args['--cmap_name']
    name = args['-o']
    display = args['--display']
    knn = int(args['--knn'])
    k_quick_shift = int(args['--k_quick_shift'])
    knn_outlier = int(args['--knn_outlier'])
    class_bias = float(args['--class_bias'])
    correlation_transformation = args['--correlation_transformation']
    figure_size = int(args['--figure_size'])

    # output setup
    timestamp = time.strftime('%Y_%m_%d_%H_%M_%S')
    dir_name = name + '_' + timestamp
    if not os.path.exists(dir_name):
        os.mkdir(dir_name)

    # logger
    if args['--verbose']:
        verbosity = 2
    else:
        verbosity = 1
    configure_logging(logger,
                      verbosity=verbosity,
                      filename=os.path.join(dir_name, 'log'))
    logger.debug(serialize_dict(args))

    # process data
    data_matrix, y, target_dict = pre_process(data_fname,
                                              target_fname,
                                              correlation_transformation,
                                              min_threshold,
                                              max_threshold)

    # run embedder
    logger.info('Writing to files in directory: %s' % dir_name)
    file_name = os.path.join(dir_name, 'img')

    embedder = Embedder(
        knn=knn,
        k_quick_shift=k_quick_shift,
        class_bias=class_bias,
        knn_outlier=knn_outlier)
    data_matrix = embedder.transform(data_matrix=data_matrix, target=y)

    # make images
    embedder.display(target_dict=target_dict, display=display,
                     display_hull=True, display_links=True,
                     display_outliers=True,
                     cmap=cmap_name, file_name=file_name,
                     figure_size=figure_size)

    # write additional files
    out_fname = os.path.join(dir_name, '2D_coords.txt')
    logger.info('Writing 2D coordinates to file: %s' % out_fname)
    with open(out_fname, 'w') as f:
        for row in embedder.embedded_data_matrix:
            for val in row:
                f.write('%.4f ' % val)
            f.write('\n')

    out_fname = os.path.join(dir_name, 'classes.txt')
    logger.info('Writing classes to file: %s' % out_fname)
    with open(out_fname, 'w') as f:
        for t in embedder.target:
            f.write('%d\n' % t)


if __name__ == '__main__':
    args = docopt(__doc__, version='graph_embed 2.0')
    main(args)
