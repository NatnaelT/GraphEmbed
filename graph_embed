#!/usr/bin/env python


"""GraphEmbed.

Compute a 2D embedding of a data matrix given supervised class information.
Instances are materialized as nodes in a graph where edges connect the
nearest neighbors. Additional invisible nodes are placed to represent the
supervised classes and instances are linked to their respective classes.
The final embedding is obtained using the spring layout algorithm presented in:
Tomihisa Kamada, and Satoru Kawai. "An algorithm for drawing general
undirected graphs." Information processing letters 31, no. 1 (1989): 7-15.

Version: 1.0
Author: Fabrizio Costa [costa@informatik.uni-freiburg.de]

Usage:
  graph_embed -i FILE (-t FILE | -n N)  [-o NAME] [--cmap_name=NAME]
              [(-m N | --min_threshold=N)] [--multi_class_threshold=N]
              [--multi_class_bias=N] [--true_class_threshold=N]
              [--true_class_bias=N] [--nearest_neighbors_threshold=N]
              [--correlation_transformation]
              [--display] [--verbose]
  graph_embed (-h | --help)
  graph_embed --version

Options:
  -i FILE                           Specify input data file.
  -t FILE                           Specify target data file.
  -n N                              Specify the num of classes [default: 1].
  -o NAME                           Prefix for output files [default: out].
  --display                         Display graphs.
  -m N, --min_threshold=N           Min num of elements per class [default: 5].
  --cmap_name=NAME                  Color scheme [default: gist_ncar].
  --correlation_transformation      Convert data matrix to corr coeff matrix.
  --nearest_neighbors_threshold=N   Number of neighbors [default: 5].
  --true_class_bias=N               Bias for clustering [default: 0.9].
  --true_class_threshold=N          Threshold for clusters [default: 3].
  --multi_class_bias=N              Multiclass bias [default: 0].
  --multi_class_threshold=N         Multiclass threshold [default: 3].
  -h --help                         Show this screen.
  --version                         Show version.
  --verbose                         Print more text.


"""

import sys
import time
import logging
import logging.handlers

import collections
from docopt import docopt
import numpy as np

from sklearn.preprocessing import LabelEncoder

from graph_layout_embedder import Embedder
from graph_layout_embedder import serialize_dict


logger = logging.getLogger(__name__)


def configure_logging(logger, verbosity=0, filename=None):
    """Utility to configure the logging aspects.

    If filename is None then
    no info is stored in files.
    If filename is not None then everything that is logged is dumped to
    file (including program traces).
    Verbosity is an int that can take values:
    0 -> warning, 1 -> info, >=2 -> debug.
    All levels are displayed on stdout, not on stderr.
    Please use exceptions and asserts
    to output on stderr.
    """
    logger.propagate = False
    logger.handlers = []
    log_level = logging.WARNING
    if verbosity == 1:
        log_level = logging.INFO
    elif verbosity >= 2:
        log_level = logging.DEBUG
    logger.setLevel(logging.DEBUG)
    # create console handler
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(log_level)
    # create formatter
    cformatter = logging.Formatter('%(message)s')
    # add formatter to ch
    ch.setFormatter(cformatter)
    # add handlers to logger
    logger.addHandler(ch)

    if filename is not None:
        # create a file handler
        fh = logging.handlers.RotatingFileHandler(filename=filename,
                                                  maxBytes=10000000,
                                                  backupCount=10)
        fh.setLevel(logging.DEBUG)
        # create formatter
        fformatter = logging.Formatter('%(asctime)s | %(levelname)-6s | %(name)10s | %(filename)10s |\
   %(lineno)4s | %(message)s')
        # add formatter to fh
        fh.setFormatter(fformatter)
        # add handlers to logger
        logger.addHandler(fh)


def _load_data_matrix(fname):
    print('Reading data from file: %s' % fname)
    data_matrix_original = []
    instance_names = []
    gene_names = []
    with open(fname) as f:
        for i, line in enumerate(f):
            if i == 0:
                instance_names = line.strip().split()[1:]
            if i > 0:
                tokens = line.strip().split('\t')
                gene_names.append(tokens[0])
                value_list = tokens[1:]
                vals = [float(j) for j in value_list]
                data_matrix_original.append(vals)
    data_matrix = np.array(data_matrix_original).T
    rows, cols = data_matrix.shape
    logger.info('#instances:%d  #features:%d' % (rows, cols))
    return data_matrix, gene_names, instance_names


def _load_target(fname):
    logger.info('Reading data from file: %s' % fname)
    targets = []
    instance_names = []
    with open(fname) as f:
        for i, line in enumerate(f):
            tokens = line.strip().split()
            instance_names.append(tokens[0])
            targets.append(tokens[1])
    logger.info('read %d values ' % len(targets))
    target_names = list(sorted(set(targets)))
    lenc = LabelEncoder()
    y = lenc.fit_transform(targets)
    targets = np.array(y)
    return targets, target_names


def _select_targets(y, min_threshold=10, max_threshold=None):
    """_select_targets.

    Return the set of targets that are occurring a number of times bounded
    by min_threshold and max_threshold.
    """
    c = collections.Counter(y)
    y_sel = []
    for y_id in c:
        if c[y_id] > min_threshold:
            if max_threshold:
                if c[y_id] < max_threshold:
                    y_sel.append(y_id)
            else:
                y_sel.append(y_id)
    return y_sel


def _filter_dataset(data_matrix, y, y_sel):
    """_filter_dataset.

    Filter data matrix and target vector selecting only instances that
    belong to y_sel.
    """
    targets = []
    instances = []
    for target, instance in zip(y, data_matrix):
        if target in y_sel:
            targets.append(target)
            instances.append(instance)
    y = np.array(np.hstack(targets))
    data_matrix = np.array(np.vstack(instances))
    return data_matrix, y


def main(args):
    """Main."""
    logger.debug(serialize_dict(args))

    # setup variables
    data_fname = args['-i']
    target_fname = args['-t']
    n_clusters = int(args['-n'])
    min_threshold = int(args['--min_threshold'])
    cmap_name = args['--cmap_name']
    name = args['-o']
    display = args['--display']
    nearest_neighbors_threshold = int(args['--nearest_neighbors_threshold'])
    true_class_bias = float(args['--true_class_bias'])
    true_class_threshold = int(args['--true_class_threshold'])
    multi_class_bias = float(args['--multi_class_bias'])
    multi_class_threshold = int(args['--multi_class_threshold'])
    correlation_transformation = args['--correlation_transformation']

    # load data
    data_matrix, gene_names, instance_names = _load_data_matrix(data_fname)

    if n_clusters == 1:
        y_orig, target_names = _load_target(target_fname)
        y_sel = _select_targets(y_orig, min_threshold=min_threshold)
        logger.info('selected %d classes with more than %d instances' %
                    (len(y_sel), min_threshold))
        data_matrix, y_orig_sel = _filter_dataset(data_matrix, y_orig, y_sel)
        rows, cols = data_matrix.shape
        logger.info('#instances:%d  #features:%d' % (rows, cols))
        logger.info('#targets: %d' % len(y_orig_sel))
        logger.info('#instances:%d  #features:%d' % (rows, cols))
        lenc = LabelEncoder()
        y = lenc.fit_transform(y_orig_sel)
        y = np.array(y)
        target_dict = dict()
        for i, c in enumerate(lenc.classes_):
            target_dict[i] = target_names[c]
            print '%d -> %d   %s' % (i, c, target_names[c])

    # prepare data matrix
    if correlation_transformation:
        _data_matrix = np.corrcoef(data_matrix)
    else:
        _data_matrix = data_matrix
    rows, cols = _data_matrix.shape
    logger.info('#instances:%d  #features:%d' % (rows, cols))

    # run embedder
    timestamp = time.strftime('%Y_%m_%d_%H_%M_%S')
    file_name = name + '_' + timestamp
    logger.info('Writing to files with prefix: %s' % file_name)

    embedder = Embedder(
        nearest_neighbors_threshold=nearest_neighbors_threshold,
        true_class_bias=true_class_bias,
        true_class_threshold=true_class_threshold,
        multi_class_bias=multi_class_bias,
        multi_class_threshold=multi_class_threshold)

    if n_clusters == 1:
        data_matrix = embedder.fit_transform(
            data_matrix=_data_matrix, target=y)
        embedder.display(target_dict=target_dict, display=display,
                         display_class_graph=True, display_clean=True,
                         cmap=cmap_name, file_name=file_name, figure_size=11)
    else:
        data_matrix = embedder.fit_transform(
            data_matrix=_data_matrix, n_clusters=n_clusters)
        embedder.display(display=display,
                         display_class_graph=True, display_clean=True,
                         cmap=cmap_name, file_name=file_name, figure_size=11)
    # write prediction files
    out_fname = file_name + '_2D_coords.txt'
    logger.info('Writing 2D coordinates to file: %s' % out_fname)
    with open(out_fname, 'w') as f:
        for row in embedder.embedded_data_matrix:
            for val in row:
                f.write('%.4f ' % val)
            f.write('\n')

    out_fname = file_name + '_target.txt'
    logger.info('Writing targets to file: %s' % out_fname)
    with open(out_fname, 'w') as f:
        for t in embedder.target:
            f.write('%d\n' % t)

    out_fname = file_name + '_probs.txt'
    logger.info('Writing probabilities to file: %s' % out_fname)
    with open(out_fname, 'w') as f:
        for prob in embedder.probs:
            for p in prob:
                f.write('%.4f ' % p)
            f.write('\n')

if __name__ == '__main__':
    args = docopt(__doc__, version='graph_embed 1.0')
    if args['--verbose']:
        verbosity = 2
    else:
        verbosity = 1
    configure_logging(logger,
                      verbosity=verbosity,
                      filename='graph_embed.log')
    main(args)
